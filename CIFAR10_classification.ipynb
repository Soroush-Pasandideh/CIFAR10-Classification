{"cells":[{"cell_type":"markdown","metadata":{"id":"boQnf0_4RlEc"},"source":["## connect to github"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25028,"status":"ok","timestamp":1723713813130,"user":{"displayName":"Soroush Pasandideh","userId":"03105788374637751870"},"user_tz":-210},"id":"amFwKeJqZgPG","outputId":"111b6866-8c0a-439a-bd47-dfc497991f23"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1723713813131,"user":{"displayName":"Soroush Pasandideh","userId":"03105788374637751870"},"user_tz":-210},"id":"g-gZidEFLinc","outputId":"32e15796-9d73-4ae5-95d4-8129f8b6ca8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/CIFAR10\n"]}],"source":["%cd /content/drive/MyDrive/CIFAR10/CIFAR10-Classification"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":335,"status":"ok","timestamp":1723713827745,"user":{"displayName":"Soroush Pasandideh","userId":"03105788374637751870"},"user_tz":-210},"id":"Es-436QhaaKQ","outputId":"da5488f7-3524-4fc9-a85d-225d7442bfea"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/CIFAR10'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}],"source":["%pwd"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"QmfHHGPoRjPl","executionInfo":{"status":"ok","timestamp":1723713830526,"user_tz":-210,"elapsed":441,"user":{"displayName":"Soroush Pasandideh","userId":"03105788374637751870"}}},"outputs":[],"source":["!git config --global user.email \"soroush.pasandideh80@gmail.com\"\n","!git config --global user.name \"Soroush Pasandideh\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23427,"status":"ok","timestamp":1723563996701,"user":{"displayName":"Soroush Pasandideh","userId":"03105788374637751870"},"user_tz":-210},"id":"SllCO9-cL5fs","outputId":"94cc2561-ee60-44bd-e1e7-a959ecd51052"},"outputs":[{"name":"stdout","output_type":"stream","text":["GitHub token: ··········\n","fatal: destination path 'CIFAR10-Classification' already exists and is not an empty directory.\n"]}],"source":["# from getpass import getpass\n","# token = getpass(\"GitHub token: \")\n","\n","# !git clone https://soroush-pasandideh:{token}@github.com/soroush-pasandideh/CIFAR10-Classification.git"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5465,"status":"ok","timestamp":1723713841760,"user":{"displayName":"Soroush Pasandideh","userId":"03105788374637751870"},"user_tz":-210},"id":"iEQkWNhWaVLf","outputId":"1ace0b21-a4d6-4650-aa5d-0657ce8a5987"},"outputs":[{"output_type":"stream","name":"stdout","text":["On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git restore <file>...\" to discard changes in working directory)\n","\t\u001b[31mmodified:   CIFAR10_classification.ipynb\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"]}],"source":["!git status"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2357,"status":"ok","timestamp":1723717544136,"user":{"displayName":"Soroush Pasandideh","userId":"03105788374637751870"},"user_tz":-210},"id":"La6YtRnvS88Q","outputId":"b087d119-e9f7-406e-c69a-bb80447046e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["[main a194b81] convert training part to a fucntion\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"," rewrite CIFAR10_classification.ipynb (77%)\n","Enumerating objects: 5, done.\n","Counting objects: 100% (5/5), done.\n","Delta compression using up to 2 threads\n","Compressing objects: 100% (3/3), done.\n","Writing objects: 100% (3/3), 1.11 KiB | 51.00 KiB/s, done.\n","Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n","remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n","remote: This repository moved. Please use the new location:\u001b[K\n","remote:   https://github.com/Soroush-Pasandideh/CIFAR10-Classification.git\u001b[K\n","To https://github.com/soroush-pasandideh/CIFAR10-Classification.git\n","   5a9b751..a194b81  main -> main\n"]}],"source":["!git add .\n","!git commit -m \"implement te\"\n","!git push origin main"]},{"cell_type":"markdown","metadata":{"id":"yezN8nZrRqpQ"},"source":["## CIFAR10"]},{"cell_type":"markdown","metadata":{"id":"adUwLx2tdsyK"},"source":["### imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZZoRpiGKdh-l"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"F1ftIMHMdmmX","executionInfo":{"status":"ok","timestamp":1723715227597,"user_tz":-210,"elapsed":465,"user":{"displayName":"Soroush Pasandideh","userId":"03105788374637751870"}}},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, random_split\n","from torchvision.datasets import CIFAR10"]},{"cell_type":"markdown","metadata":{"id":"nTjAFiN9eXZn"},"source":["### define a transformer to normalize data while loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_0T-D3PkepWT"},"outputs":[],"source":["transform = transforms.Compose(\n","    [\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n","    ]\n",")"]},{"cell_type":"markdown","metadata":{"id":"MjhTzoFVgMMW"},"source":["### load dataset from torchvision"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3dGTRlhSgKCc","executionInfo":{"status":"ok","timestamp":1723715798549,"user_tz":-210,"elapsed":4656,"user":{"displayName":"Soroush Pasandideh","userId":"03105788374637751870"}},"outputId":"02f53c9a-c948-403e-9a0b-dff969ef7fe0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}],"source":["batch_size = 64\n","\n","trainset = CIFAR10(root='./data', train=True,\n","                                        download=True,transform=transform)\n","# separate train and validation data\n","train_size = int(0.9 * len(trainset))\n","val_size = len(trainset) - train_size\n","train_subset, val_subset = random_split(trainset, [train_size, val_size])\n","\n","train_loader = DataLoader(train_subset, batch_size=batch_size,\n","                         shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_subset, batch_size=batch_size,\n","                        shuffle=False, num_workers=2)\n","\n","testset = CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","\n","test_loader = DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"_dml1EquJ54c"},"source":["#### analyse data"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ENSagE1Vk3cj","executionInfo":{"status":"ok","timestamp":1723715798549,"user_tz":-210,"elapsed":4,"user":{"displayName":"Soroush Pasandideh","userId":"03105788374637751870"}},"outputId":"9108462e-b9e8-474f-c1e7-6a1e4fd80295"},"outputs":[{"output_type":"stream","name":"stdout","text":["train_subset size: 45000\n","train_loader size: 704\n","testset size: 10000\n","test_loader size: 157\n","valset size: 5000\n","val_loader size: 79\n"]}],"source":["print('train_subset size:', len(train_subset))\n","print('train_loader size:', len(train_loader))\n","print('testset size:', len(testset))\n","print('test_loader size:', len(test_loader))\n","print('valset size:', len(val_subset))\n","print('val_loader size:', len(val_loader))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hzzdCsOHDODf","executionInfo":{"status":"ok","timestamp":1723712359701,"user_tz":-210,"elapsed":12,"user":{"displayName":"Soroush Pasandideh","userId":"03105788374637751870"}},"outputId":"8e5b544f-7c2b-49b9-9da9-619370478b01"},"outputs":[{"output_type":"stream","name":"stdout","text":["count of batches: 12500\n","count of all data in train dataloader: 50000\n","shape of first ekement -> data, label: 2\n","shape of the first data: torch.Size([3, 32, 32])\n"]}],"source":["print('count of batches:', len(train_loader))\n","print('count of all data in train dataloader:', len(train_loader.dataset))\n","print('shape of first ekement -> data, label:', len(train_loader.dataset[0]))\n","print('shape of the first data:', train_loader.dataset[0][0].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bSNy_mp9iVcs"},"outputs":[],"source":["classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"]},{"cell_type":"markdown","metadata":{"id":"EOtF5i5DlsJ3"},"source":["### implement CNN"]},{"cell_type":"code","source":["!pip install torchinfo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SlQkg5zKRZv5","executionInfo":{"status":"ok","timestamp":1723712535316,"user_tz":-210,"elapsed":5911,"user":{"displayName":"Soroush Pasandideh","userId":"03105788374637751870"}},"outputId":"6d1707d2-abf8-4769-c613-ce1ed0bca7cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"]}]},{"cell_type":"code","execution_count":52,"metadata":{"id":"xUNHJwHrlwnx","executionInfo":{"status":"ok","timestamp":1723719968823,"user_tz":-210,"elapsed":352,"user":{"displayName":"Soroush Pasandideh","userId":"03105788374637751870"}}},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as func\n","import torch.optim as optim\n","from torchinfo import summary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TK8wKL2LmDTq"},"outputs":[],"source":["class CNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(func.relu(self.conv1(x)))\n","        x = self.pool(func.relu(self.conv2(x)))\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = func.relu(self.fc1(x))\n","        x = func.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x"]},{"cell_type":"code","source":["cnn = CNN()\n","summary(cnn, input_size=(1, 3, 32, 32))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sCOHcnaYQ4PM","executionInfo":{"status":"ok","timestamp":1723713892286,"user_tz":-210,"elapsed":352,"user":{"displayName":"Soroush Pasandideh","userId":"03105788374637751870"}},"outputId":"48afb77f-3a87-4a04-a5a5-5b8016259c92"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","CNN                                      [1, 10]                   --\n","├─Conv2d: 1-1                            [1, 6, 28, 28]            456\n","├─MaxPool2d: 1-2                         [1, 6, 14, 14]            --\n","├─Conv2d: 1-3                            [1, 16, 10, 10]           2,416\n","├─MaxPool2d: 1-4                         [1, 16, 5, 5]             --\n","├─Linear: 1-5                            [1, 120]                  48,120\n","├─Linear: 1-6                            [1, 84]                   10,164\n","├─Linear: 1-7                            [1, 10]                   850\n","==========================================================================================\n","Total params: 62,006\n","Trainable params: 62,006\n","Non-trainable params: 0\n","Total mult-adds (M): 0.66\n","==========================================================================================\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 0.05\n","Params size (MB): 0.25\n","Estimated Total Size (MB): 0.31\n","=========================================================================================="]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["### using GPU"],"metadata":{"id":"sOLzETjxft0-"}},{"cell_type":"code","source":["use_gpu = True\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n","# device = torch.device(\"cpu\")\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5NYRXCLfthf","executionInfo":{"status":"ok","timestamp":1723716282970,"user_tz":-210,"elapsed":454,"user":{"displayName":"Soroush Pasandideh","userId":"03105788374637751870"}},"outputId":"a859aca5-ce59-402e-e3cc-29ae8e9438a8"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":46}]},{"cell_type":"markdown","metadata":{"id":"7af8I00X88Hj"},"source":["### train one epoch"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"HA9fSmM58_Oj","executionInfo":{"status":"ok","timestamp":1723717649908,"user_tz":-210,"elapsed":3,"user":{"displayName":"Soroush Pasandideh","userId":"03105788374637751870"}}},"outputs":[],"source":["def train_one_epoch(model: nn.Module, optim: torch.optim.Optimizer,\n","                    trainloader: DataLoader, loss_func, device:torch.device):\n","\n","    num_samples = len(train_loader.dataset)\n","    num_batches = len(train_loader)\n","\n","    running_corrects = 0\n","    running_loss = 0.0\n","\n","    model.train()\n","    for batch_number, (inputs, targets) in enumerate(trainloader):\n","        inputs = inputs.to(device)\n","        targets = targets.to(device)\n","\n","        # forward pass\n","        outputs = model(inputs)\n","\n","        # backward\n","        loss = loss_func(outputs, targets)\n","        loss.backward()\n","\n","        # optimize\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        # statistics\n","        _, preds = torch.max(outputs, dim=1)\n","        running_corrects += torch.sum(preds == targets).cpu()\n","        running_loss += loss.item()\n","\n","    epoch_train_acc = (running_corrects/num_samples)*100\n","    epoch_train_loss = (running_loss/num_batches) # it's the mean of all losses\n","    print(f\"train:\\n corrects: {running_corrects} of {num_samples}\")\n","\n","    return epoch_train_acc, epoch_train_loss"]},{"cell_type":"markdown","source":["### test model"],"metadata":{"id":"WxGYgBdfii0x"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Racx-Vv4Rly1"},"outputs":[],"source":["def test_model(model: nn.Module, dataloader: DataLoader,\n","                       loss_fn, device: torch.device):\n","\n","    num_samples = len(train_loader.dataset)\n","    num_batches = len(train_loader)\n","\n","    running_corrects = 0\n","    running_loss = 0.0\n","\n","    true_labels = torch.empty(0).to(device)\n","    pred_labels = torch.empty(0).to(device)\n","\n","    # we call `model.eval()` to set dropout and batch normalization layers\n","    # to evaluation mode before running inference.\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for batch_number, (inputs, targets) in enumerate(dataloader):\n","            inputs = inputs.to(device)\n","            targets = targets.to(device)\n","\n","            # forward pass\n","            outputs = model(inputs)\n","\n","            # backward\n","            loss = loss_func(outputs, targets)\n","\n","            # statistics\n","            _, preds = torch.max(outputs, dim=1)\n","            running_corrects += torch.sum(preds == targets).cpu()\n","            running_loss += loss.item()\n","\n","            true_labels = torch.cat([true_labels, targets], dim=0)\n","            pred_labels = torch.cat([pred_labels, preds], dim=0)\n","\n","    epoch_test_acc = (running_corrects/num_samples)*100\n","    epoch_test_loss = (running_loss/num_batches) # it's the mean of all losses\n","    print(f\"train:\\n corrects: {running_corrects} of {num_samples}\")\n","\n","    return epoch_test_acc, epoch_test_loss, true_labels, pred_labels"]},{"cell_type":"markdown","source":["### train and validate model"],"metadata":{"id":"r7xNuk7HsJE9"}},{"cell_type":"code","source":["def train_validate(train_loader, val_loader, model, device):\n","    num_epochs = 5\n","    learning_rate = 0.01\n","\n","    full_dataloader = {\n","        'train': train_loader,\n","        'val': val_loader\n","    }\n","\n","    cnn = model\n","    cnn = cnn.to(device)\n","\n","    loss_func = nn.CrossEntropyLoss()\n","    optimizer = optim.SGD(cnn.parameters(), lr=learning_rate, momentum=0.9)\n",""],"metadata":{"id":"tVAwbp57sQgM"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}